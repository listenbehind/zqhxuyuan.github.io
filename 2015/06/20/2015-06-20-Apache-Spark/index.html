<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Apache Spark入门 | zqhxuyuan</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Apache Spark小白入门教程">
<meta property="og:type" content="article">
<meta property="og:title" content="Apache Spark入门">
<meta property="og:url" content="http://github.com/zqhxuyuan/2015/06/20/2015-06-20-Apache-Spark/index.html">
<meta property="og:site_name" content="zqhxuyuan">
<meta property="og:description" content="Apache Spark小白入门教程">
<meta property="og:image" content="http://7xjs7x.com1.z0.glb.clouddn.com/spark1.png">
<meta property="og:image" content="http://7xjs7x.com1.z0.glb.clouddn.com/spark2.png">
<meta property="og:image" content="http://7xjs7x.com1.z0.glb.clouddn.com/spark4.png">
<meta property="og:image" content="http://7xjs7x.com1.z0.glb.clouddn.com/spark5.png">
<meta property="og:image" content="http://7xjs7x.com1.z0.glb.clouddn.com/spark7.png">
<meta property="og:image" content="http://7xjs7x.com1.z0.glb.clouddn.com/spark6.png">
<meta property="og:image" content="http://7xjs7x.com1.z0.glb.clouddn.com/spark3.png">
<meta property="og:image" content="http://img.blog.csdn.net/20170713192930380">
<meta property="og:image" content="http://7xjs7x.com1.z0.glb.clouddn.com/spark8.png">
<meta property="og:image" content="http://7xjs7x.com1.z0.glb.clouddn.com/sha1.png">
<meta property="og:image" content="http://7xjs7x.com1.z0.glb.clouddn.com/sha2.png">
<meta property="og:image" content="http://7xjs7x.com1.z0.glb.clouddn.com/sha3.png">
<meta property="og:image" content="http://7xjs7x.com1.z0.glb.clouddn.com/spark-perf.png">
<meta property="og:updated_time" content="2017-07-17T01:23:04.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Apache Spark入门">
<meta name="twitter:description" content="Apache Spark小白入门教程">
  
    <link rel="alternative" href="/atom.xml" title="zqhxuyuan" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" href="/css/style.css" type="text/css">
  <link rel="stylesheet" href="/font-awesome/css/font-awesome.min.css">
  <link rel="apple-touch-icon" href="/apple-touch-icon.png">
</head>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img lazy-src="https://avatars1.githubusercontent.com/u/1088525?v=3&amp;s=180" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">任何忧伤,都抵不过世界的美丽</a></h1>
		</hgroup>

		
				


		
			<div id="switch-btn" class="switch-btn">
				<div class="icon">
					<div class="icon-ctn">
						<div class="icon-wrap icon-house" data-idx="0">
							<div class="birdhouse"></div>
							<div class="birdhouse_holes"></div>
						</div>
						<div class="icon-wrap icon-ribbon hide" data-idx="1">
							<div class="ribbon"></div>
						</div>
						
						
						<div class="icon-wrap icon-me hide" data-idx="3">
							<div class="user"></div>
							<div class="shoulder"></div>
						</div>
						
					</div>
					
				</div>
				<div class="tips-box hide">
					<div class="tips-arrow"></div>
					<ul class="tips-inner">
						<li>菜单</li>
						<li>标签</li>
						
						
						<li>关于我</li>
						
					</ul>
				</div>
			</div>
		

		<div id="switch-area" class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/">主页</a></li>
				        
							<li><a href="/archives/">归档</a></li>
				        
							<li><a href="/tags/">标签</a></li>
				        
							<li><a href="/about/">关于</a></li>
				        
						</ul>
					</nav>
					<nav class="header-nav">
						<ul class="social">
							
								<li id="新浪微博"><a class="新浪微博" target="_blank" href="http://weibo.com/xuyuantree" title="新浪微博"></a></li>
					        
								<li id="GitHub"><a class="GitHub" target="_blank" href="http://github.com/zqhxuyuan" title="GitHub"></a></li>
					        
								<li id="RSS"><a class="RSS" target="_blank" href="/atom.xml" title="RSS"></a></li>
					        
						</ul>
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						<a href="/tags/akka/" style="font-size: 10px;">akka</a> <a href="/tags/apex/" style="font-size: 10px;">apex</a> <a href="/tags/bigdata/" style="font-size: 10px;">bigdata</a> <a href="/tags/book/" style="font-size: 10px;">book</a> <a href="/tags/cassandra/" style="font-size: 18.75px;">cassandra</a> <a href="/tags/clojure/" style="font-size: 10px;">clojure</a> <a href="/tags/drill/" style="font-size: 16.25px;">drill</a> <a href="/tags/druid/" style="font-size: 13.75px;">druid</a> <a href="/tags/dubbo/" style="font-size: 10px;">dubbo</a> <a href="/tags/elasticsearch/" style="font-size: 10px;">elasticsearch</a> <a href="/tags/etl/" style="font-size: 10px;">etl</a> <a href="/tags/geode/" style="font-size: 10px;">geode</a> <a href="/tags/graph/" style="font-size: 13.75px;">graph</a> <a href="/tags/hadoop/" style="font-size: 11.25px;">hadoop</a> <a href="/tags/hbase/" style="font-size: 15px;">hbase</a> <a href="/tags/ignite/" style="font-size: 10px;">ignite</a> <a href="/tags/java/" style="font-size: 10px;">java</a> <a href="/tags/jvm/" style="font-size: 10px;">jvm</a> <a href="/tags/kafka/" style="font-size: 20px;">kafka</a> <a href="/tags/midd/" style="font-size: 11.25px;">midd</a> <a href="/tags/ops/" style="font-size: 13.75px;">ops</a> <a href="/tags/redis/" style="font-size: 11.25px;">redis</a> <a href="/tags/rocketmq/" style="font-size: 10px;">rocketmq</a> <a href="/tags/scala/" style="font-size: 13.75px;">scala</a> <a href="/tags/spark/" style="font-size: 17.5px;">spark</a> <a href="/tags/storm/" style="font-size: 17.5px;">storm</a> <a href="/tags/timeseries/" style="font-size: 12.5px;">timeseries</a> <a href="/tags/work/" style="font-size: 13.75px;">work</a> <a href="/tags/流处理/" style="font-size: 11.25px;">流处理</a>
					</div>
				</section>
				
				
				

				
				
				<section class="switch-part switch-part3">
				
					<div id="js-aboutme">BIG(DATA)</div>
				</section>
				
			</div>
		</div>
	</header>				
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">任何忧伤,都抵不过世界的美丽</a></h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<a href="/" class="profilepic">
				<img lazy-src="https://avatars1.githubusercontent.com/u/1088525?v=3&amp;s=180" class="js-avatar">
			</a>
			<hgroup>
			  <h1 class="header-author"><a href="/" title="回到主页">任何忧伤,都抵不过世界的美丽</a></h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">主页</a></li>
		        
					<li><a href="/archives/">归档</a></li>
		        
					<li><a href="/tags/">标签</a></li>
		        
					<li><a href="/about/">关于</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
						<ul class="social">
							
								<li id="新浪微博"><a class="新浪微博" target="_blank" href="http://weibo.com/xuyuantree" title="新浪微博"></a></li>
					        
								<li id="GitHub"><a class="GitHub" target="_blank" href="http://github.com/zqhxuyuan" title="GitHub"></a></li>
					        
								<li id="RSS"><a class="RSS" target="_blank" href="/atom.xml" title="RSS"></a></li>
					        
						</ul>
			</nav>
		</header>				
	</div>
</nav>
      <div class="body-wrap"><article id="post-2015-06-20-Apache-Spark" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2015/06/20/2015-06-20-Apache-Spark/" class="article-date">
  	<time datetime="2015-06-19T16:00:00.000Z" itemprop="datePublished">2015-06-20</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Apache Spark入门
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        
	<div class="article-category tagcloud">
	<a class="article-category-link" href="/categories/bigdata/">bigdata</a>
	</div>


        
	<div class="article-tag tagcloud">
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>
	</div>

        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <p>Apache Spark小白入门教程</p>
<a id="more"></a>
<h2 id="Spark本地模式快速（十秒钟）入门">Spark本地模式快速（十秒钟）入门</h2><blockquote>
<p>参考：<a href="http://spark.apache.org/docs/latest/quick-start.html" target="_blank" rel="external">http://spark.apache.org/docs/latest/quick-start.html</a></p>
</blockquote>
<p>解压缩spark包，在本地测试，不需要安装hadoop，直接启动<code>spark-shell</code></p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">➜  spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span>  bin/spark-shell</span><br><span class="line">Using Scala version <span class="number">2.10</span><span class="number">.4</span> (Java HotSpot(TM) <span class="number">64</span>-Bit Server VM, Java <span class="number">1.8</span><span class="number">.0</span>_25)</span><br><span class="line"><span class="number">15</span><span class="regexp">/06/</span><span class="number">28</span> <span class="number">10</span>:<span class="number">36</span>:<span class="number">07</span> INFO ui.<span class="string">SparkUI:</span> Started SparkUI at <span class="string">http:</span><span class="comment">//127.0.0.1:4040</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/06/</span><span class="number">28</span> <span class="number">10</span>:<span class="number">36</span>:<span class="number">07</span> INFO repl.<span class="string">SparkILoop:</span> Created spark context..</span><br><span class="line">Spark context available <span class="keyword">as</span> sc.</span><br><span class="line"><span class="number">15</span><span class="regexp">/06/</span><span class="number">28</span> <span class="number">10</span>:<span class="number">36</span>:<span class="number">08</span> INFO hive.<span class="string">HiveContext:</span> Initializing execution hive, version <span class="number">0.13</span><span class="number">.1</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/06/</span><span class="number">28</span> <span class="number">10</span>:<span class="number">36</span>:<span class="number">23</span> INFO repl.<span class="string">SparkILoop:</span> Created sql context (with Hive support)..</span><br><span class="line">SQL context available <span class="keyword">as</span> sqlContext.</span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure>
<p>spark-shell默认创建了<code>SparkContext sc</code>和<code>SqlContext sqlContext</code>，下面开始试验一些RDD操作</p>
<p>第一个例子: 统计一个文本文件的单词数量.调用<code>sc.textFile(fileName)</code>会生成一个<code>MapPartitionsRDD</code>  </p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val textFile = sc.<span class="function"><span class="title">textFile</span><span class="params">(<span class="string">"README.md"</span>)</span></span></span><br><span class="line"><span class="number">15</span>/<span class="number">06</span>/<span class="number">28</span> <span class="number">10</span>:<span class="number">36</span>:<span class="number">45</span> INFO storage<span class="class">.MemoryStore</span>: <span class="function"><span class="title">ensureFreeSpace</span><span class="params">(<span class="number">63424</span>)</span></span> called with curMem=<span class="number">0</span>, maxMem=<span class="number">278019440</span></span><br><span class="line"><span class="number">15</span>/<span class="number">06</span>/<span class="number">28</span> <span class="number">10</span>:<span class="number">36</span>:<span class="number">45</span> INFO storage<span class="class">.MemoryStore</span>: Block broadcast_0 stored as values <span class="keyword">in</span> memory (estimated size <span class="number">61.9</span> KB, free <span class="number">265.1</span> MB)</span><br><span class="line"><span class="number">15</span>/<span class="number">06</span>/<span class="number">28</span> <span class="number">10</span>:<span class="number">36</span>:<span class="number">45</span> INFO storage<span class="class">.MemoryStore</span>: <span class="function"><span class="title">ensureFreeSpace</span><span class="params">(<span class="number">20061</span>)</span></span> called with curMem=<span class="number">63424</span>, maxMem=<span class="number">278019440</span></span><br><span class="line"><span class="number">15</span>/<span class="number">06</span>/<span class="number">28</span> <span class="number">10</span>:<span class="number">36</span>:<span class="number">45</span> INFO storage<span class="class">.MemoryStore</span>: Block broadcast_0_piece0 stored as bytes <span class="keyword">in</span> memory (estimated size <span class="number">19.6</span> KB, free <span class="number">265.1</span> MB)</span><br><span class="line"><span class="number">15</span>/<span class="number">06</span>/<span class="number">28</span> <span class="number">10</span>:<span class="number">36</span>:<span class="number">45</span> INFO storage<span class="class">.BlockManagerInfo</span>: Added broadcast_0_piece0 <span class="keyword">in</span> memory on localhost:<span class="number">58638</span> (size: <span class="number">19.6</span> KB, free: <span class="number">265.1</span> MB)</span><br><span class="line"><span class="number">15</span>/<span class="number">06</span>/<span class="number">28</span> <span class="number">10</span>:<span class="number">36</span>:<span class="number">45</span> INFO spark<span class="class">.SparkContext</span>: Created broadcast <span class="number">0</span> from textFile at &lt;console&gt;:<span class="number">21</span></span><br><span class="line">textFile: org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.rdd</span><span class="class">.RDD</span>[String] = MapPartitionsRDD[<span class="number">1</span>] at textFile at &lt;console&gt;:<span class="number">21</span></span><br></pre></td></tr></table></figure>
<p>调用上面生成的textFile RDD的count()会触发一个Action.  </p>
<blockquote>
<p>注意：由于本机已经安装了Hadoop,使用的是伪分布式模式,所以Spark会读取Hadoop的配置信息.<br>我们这里先不启动Hadoop,使用本地模式,要手动添加<code>file:///</code>并使用绝对路径读取文本文件.<br>重新构造读取本地文本文件的textFile RDD，并调用<code>count()</code>方法。</p>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; textFile.count()</span><br><span class="line">java.net.ConnectException: Call From hadoop/<span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span> to localhost:<span class="number">9000</span> failed on </span><br><span class="line">  connection exception: java.net.ConnectException: 拒绝连接; </span><br><span class="line">  For more details see:  http:<span class="comment">//wiki.apache.org/hadoop/ConnectionRefused</span></span><br><span class="line">	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</span><br><span class="line">    ...</span><br><span class="line">Caused by: java.net.ConnectException: 拒绝连接</span><br><span class="line">	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)</span><br><span class="line">	...</span><br><span class="line">scala&gt; textFile</span><br><span class="line">res1: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[<span class="number">1</span>] at textFile at &lt;console&gt;:<span class="number">21</span></span><br><span class="line"></span><br><span class="line">scala&gt; val textFile = sc.textFile(<span class="string">"file:///home/hadoop/soft/spark-1.4.0-bin-hadoop2.6/README.md"</span>)</span><br><span class="line">textFile: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[<span class="number">3</span>] at textFile at &lt;console&gt;:<span class="number">21</span></span><br><span class="line"></span><br><span class="line">scala&gt; textFile.count()</span><br><span class="line"><span class="number">15</span>/<span class="number">06</span>/<span class="number">28</span> <span class="number">10</span>:<span class="number">44</span>:<span class="number">07</span> INFO scheduler.DAGScheduler: Job <span class="number">0</span> finished: count at &lt;console&gt;:<span class="number">24</span>, took <span class="number">0.275609</span> s</span><br><span class="line">res2: Long = <span class="number">98</span></span><br></pre></td></tr></table></figure>
<p>又一个Action RDD : 输出文本文件的第一行  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; textFile.first()</span><br><span class="line"><span class="number">15</span>/<span class="number">06</span>/<span class="number">28</span> <span class="number">10</span>:<span class="number">44</span>:<span class="number">27</span> INFO scheduler.DAGScheduler: Job <span class="number">1</span> finished: first at &lt;console&gt;:<span class="number">24</span>, took <span class="number">0.017917</span> s</span><br><span class="line">res3: String = <span class="preprocessor"># Apache Spark</span></span><br></pre></td></tr></table></figure>
<p>统计包含了Spark这个单词一共有几行</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val linesWithSpark = textFile.<span class="function"><span class="title">filter</span><span class="params">(line =&gt; line.contains(<span class="string">"Spark"</span>)</span></span>)</span><br><span class="line">linesWithSpark: org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.rdd</span><span class="class">.RDD</span>[String] = MapPartitionsRDD[<span class="number">4</span>] at <span class="attribute">filter</span> at &lt;console&gt;:<span class="number">23</span></span><br><span class="line">scala&gt; textFile.<span class="function"><span class="title">filter</span><span class="params">(line =&gt; line.contains(<span class="string">"Spark"</span>)</span></span>).<span class="function"><span class="title">count</span><span class="params">()</span></span></span><br></pre></td></tr></table></figure>
<p>文本文件中长度最长的那一行,它一共有多少个单词</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; textFile.<span class="function"><span class="title">map</span><span class="params">(line =&gt; line.split(<span class="string">" "</span>)</span></span>.size).<span class="function"><span class="title">reduce</span><span class="params">((a, b)</span></span> =&gt; <span class="keyword">if</span> (<span class="tag">a</span> &gt; b) <span class="tag">a</span> <span class="keyword">else</span> b)</span><br></pre></td></tr></table></figure>
<p>单词计数（WordCount）</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val wordCounts = textFile.<span class="function"><span class="title">flatMap</span><span class="params">(line =&gt; line.split(<span class="string">" "</span>)</span></span>).<span class="function"><span class="title">map</span><span class="params">(word =&gt; (word, <span class="number">1</span>)</span></span>).<span class="function"><span class="title">reduceByKey</span><span class="params">((a, b)</span></span> =&gt; <span class="tag">a</span> + b)</span><br><span class="line">wordCounts: org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.rdd</span><span class="class">.RDD</span>[(String, Int)] = ShuffledRDD[<span class="number">9</span>] at reduceByKey at &lt;console&gt;:<span class="number">23</span></span><br><span class="line"></span><br><span class="line">scala&gt;  wordCounts.<span class="function"><span class="title">collect</span><span class="params">()</span></span></span><br><span class="line">res6: Array[(String, Int)] = <span class="function"><span class="title">Array</span><span class="params">((package,<span class="number">1</span>)</span></span>, (this,<span class="number">1</span>), (Version<span class="string">"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version),1), (Because,1), (Python,2), (cluster.,1), (its,1), ([run,1), (general,2), (have,1), (pre-built,1), (locally.,1), (locally,2), (changed,1), (sc.parallelize(1,1), (only,1), (several,1), (This,2), (basic,1), (Configuration,1), (learning,,1), (documentation,3), (YARN,,1), (graph,1), (Hive,2), (first,1), (["</span>Specifying,<span class="number">1</span>), (<span class="string">"yarn-client"</span>,<span class="number">1</span>), (page](http:<span class="comment">//spark.apache.org/documentation.html),1), ([params]`.,1), (application,1), ([project,2), (prefer,1), (SparkPi,2), (&lt;http://spark.apache.org/&gt;,1), (engine,1), (version,1), (file,1), (documentation,,1), (MASTER,1), (example,3), (distribution.,1), (are,1), (params,1), (scala&gt;,1), (systems.,1...</span></span><br></pre></td></tr></table></figure>
<p>RDD缓存（Cache），第一次计算花了0.05s，第二次计算的时间只有0.01s。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; linesWithSpark.cache()</span><br><span class="line">res7: linesWithSpark.type = MapPartitionsRDD[<span class="number">4</span>] at filter at &lt;console&gt;:<span class="number">23</span></span><br><span class="line"></span><br><span class="line">scala&gt; linesWithSpark.count()</span><br><span class="line"><span class="number">15</span>/<span class="number">06</span>/<span class="number">28</span> <span class="number">10</span>:<span class="number">47</span>:<span class="number">11</span> INFO scheduler.DAGScheduler: Job <span class="number">5</span> finished: count at &lt;console&gt;:<span class="number">26</span>, took <span class="number">0.054036</span> s</span><br><span class="line">res8: Long = <span class="number">19</span></span><br><span class="line"></span><br><span class="line">scala&gt; linesWithSpark.count()</span><br><span class="line"><span class="number">15</span>/<span class="number">06</span>/<span class="number">28</span> <span class="number">10</span>:<span class="number">47</span>:<span class="number">14</span> INFO scheduler.DAGScheduler: Job <span class="number">6</span> finished: count at &lt;console&gt;:<span class="number">26</span>, took <span class="number">0.016638</span> s</span><br><span class="line">res9: Long = <span class="number">19</span></span><br></pre></td></tr></table></figure>
<p>用表格的形式列举出来RDD转换操作的几个实验步骤。注意：上面只有Action操作才有编号，没有Action没有编号，比如cache()就不是Action</p>
<table>
<thead>
<tr>
<th>步骤</th>
<th>动作</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>val textFile = sc.textFile(“file:///home/hadoop/soft/spark-1.4.0-bin-hadoop2.6/README.md”) <br> <strong>textFile.count()</strong></td>
</tr>
<tr>
<td>1</td>
<td>textFile.first()</td>
</tr>
<tr>
<td>2</td>
<td>val linesWithSpark = textFile.filter(line =&gt; line.contains(“Spark”)) <br> <strong>linesWithSpark.count()</strong></td>
</tr>
<tr>
<td>3</td>
<td>textFile.map(line =&gt; line.split(“ “).size).reduce((a, b) =&gt; if (a &gt; b) a else b) <br> <strong>val wordCounts = textFile.flatMap(line =&gt; line.split(“ “)).map(word =&gt; (word, 1)).reduceByKey((a, b) =&gt; a + b)</strong></td>
</tr>
<tr>
<td>4</td>
<td><strong>wordCounts.collect()</strong></td>
</tr>
<tr>
<td>5</td>
<td>linesWithSpark.cache();<br> <strong>linesWithSpark.count()</strong></td>
</tr>
<tr>
<td>6</td>
<td><strong>linesWithSpark.count()</strong></td>
</tr>
<tr>
<td>7</td>
<td><strong>linesWithSpark.count()</strong></td>
</tr>
</tbody>
</table>
<p>spark-shell启动时会打印：<code>INFO ui.SparkUI: Started SparkUI at http://127.0.0.1:4040</code>。<br>打开<a href="http://127.0.0.1:4040" target="_blank" rel="external">http://127.0.0.1:4040</a>，这个页面是Spark的WebUI页面。最主要的有三个Tab：Jobs, Stages, Storage。</p>
<h3 id="Jobs_&amp;_Stages">Jobs &amp; Stages</h3><p><strong>Jobs:</strong> 上面每个Action RDD编号对应了下图中的Job Id.  </p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/spark1.png" alt=""></p>
<p><strong>Stages:</strong> 上面有8个Job, 但是Stages多了一个(一共有9个Stages). 其实是④的<code>collect()</code>有两个stage  </p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/spark2.png" alt=""></p>
<p>在Jobs中点击Job Id=4的collect RDD(输出WordCount的结果). 在下方的列表中可以看到有2个Stages<br>仔细观察列表的最后面两列, 分别是Shuffle Read和Shuffle Write.<br>其中map会进行Shuffle Write, collect会进行Shuffle Read</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/spark4.png" alt=""></p>
<p>点击Stage Id=4的map. 它的DAG可视化图和上面的概览图的左侧是一样的</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/spark5.png" alt=""></p>
<p>Spark的WebUI还提供了一个EventTime,可以很清楚地看到每个阶段消耗的时间</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/spark7.png" alt=""></p>
<p>回退,点击Stage Id=5的collect</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/spark6.png" alt=""></p>
<p><strong>Storage:</strong> 只有在<code>cache()</code>之后，执行完一次Action才有。</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/spark3.png" alt=""></p>
<hr>
<h2 id="Spark_Standalone集群安装（30分钟~1小时）">Spark Standalone集群安装（30分钟~1小时）</h2><p>Standalone译为单机、独立式的，但是并不是说Standalone只有一台机器，它也可以有分布式/集群模式，主要有两个组件：</p>
<ul>
<li>Master</li>
<li>Worker</li>
</ul>
<p><img src="http://img.blog.csdn.net/20170713192930380" alt="s"></p>
<p>1.准备工作:</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">安装hadoop分布式集群，启动hdfs和yarn</span><br><span class="line">master无密码ssh到<span class="function"><span class="title">slaves</span><span class="params">(将master的pub追加到所有slaves的authorized_keys)</span></span></span><br><span class="line">关闭所有节点的防火墙(chkconfig iptables off)</span><br><span class="line">安装scala-<span class="number">2.10</span>,并设置~/.bashrc</span><br></pre></td></tr></table></figure>
<p>2.修改spark-env.sh的配置文件</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">$ vi $SPARK_HOME/conf/spark-env.sh</span><br><span class="line"><span class="preprocessor">#环境变量</span></span><br><span class="line"><span class="keyword">export</span> JAVA_HOME=/usr/java/jdk1<span class="number">.7</span><span class="number">.0</span>_51</span><br><span class="line"><span class="keyword">export</span> SCALA_HOME=/usr/install/scala-<span class="number">2.10</span><span class="number">.5</span></span><br><span class="line"></span><br><span class="line"><span class="preprocessor">#最简配置</span></span><br><span class="line"><span class="keyword">export</span> HADOOP_HOME=/usr/install/hadoop</span><br><span class="line"><span class="keyword">export</span> HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop</span><br><span class="line"><span class="keyword">export</span> SPARK_MASTER_IP=dp0652</span><br><span class="line"><span class="keyword">export</span> MASTER=spark:<span class="comment">//dp0652:7077</span></span><br><span class="line"><span class="preprocessor">#export SPARK_LOCAL_IP=dp0652</span></span><br><span class="line"><span class="keyword">export</span> SPARK_LOCAL_DIRS=/usr/install/spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span></span><br><span class="line"></span><br><span class="line"><span class="preprocessor">#其他配置</span></span><br><span class="line"><span class="keyword">export</span> SPARK_MASTER_WEBUI_PORT=<span class="number">8082</span></span><br><span class="line"><span class="keyword">export</span> SPARK_MASTER_PORT=<span class="number">7077</span></span><br><span class="line"><span class="keyword">export</span> SPARK_WORKER_CORES=<span class="number">1</span></span><br><span class="line"><span class="keyword">export</span> SPARK_WORKER_INSTANCES=<span class="number">1</span></span><br><span class="line"><span class="keyword">export</span> SPARK_WORKER_MEMORY=<span class="number">8</span>g</span><br></pre></td></tr></table></figure>
<blockquote>
<p>在新版本中，SPARK_HOME设置为Spark的安装目录，SPARK_LOCAL_DIRS为临时文件夹，存放spark的运行时作业信息。  </p>
</blockquote>
<p>有几个端口信息：</p>
<ul>
<li>8082：master web ui</li>
<li>7077：master port</li>
<li>上一节的4040是应用程序的端口，不同应用程序的端口从4040不断增加</li>
</ul>
<p>3.添加slaves文件，指定所有的Worker</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">vi</span> <span class="keyword">conf</span>/slaves</span><br><span class="line">dp0652</span><br><span class="line">dp0653</span><br><span class="line">dp0655</span><br><span class="line">dp0656</span><br><span class="line">dp0657</span><br></pre></td></tr></table></figure>
<p>4.将spark目录分发到集群的其他节点</p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd ..</span><br><span class="line">scp -r <span class="variable">$SPARK</span>_HOME dp0653:/usr/install</span><br><span class="line">scp -r <span class="variable">$SPARK</span>_HOME dp0655:/usr/install</span><br><span class="line">scp -r <span class="variable">$SPARK</span>_HOME dp0656:/usr/install</span><br><span class="line">scp -r <span class="variable">$SPARK</span>_HOME dp0657:/usr/install</span><br></pre></td></tr></table></figure>
<p>由于集群中dp0652和dp0653的内存比较大, 我们修改了这两个节点的spark-env.sh,并且启动了两个Worker示例。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">export</span> SPARK_WORKER_INSTANCES=<span class="number">2</span></span><br><span class="line"><span class="keyword">export</span> SPARK_WORKER_MEMORY=<span class="number">20</span>g</span><br></pre></td></tr></table></figure>
<p>5.启动集群, 在master上执行<code>start-all.sh</code>就可以启动Spark Master和所有的Worker.  </p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0652 spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>]$ sbin/start-all<span class="class">.sh</span></span><br><span class="line">starting org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.master</span><span class="class">.Master</span>, logging to /usr/install/spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>/sbin/../logs/spark-qihuang<span class="class">.zheng-org</span><span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.master</span><span class="class">.Master-1-dp0652</span><span class="class">.out</span></span><br><span class="line">dp0656: starting org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker</span>, logging to /usr/install/spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>/sbin/../logs/spark-qihuang<span class="class">.zheng-org</span><span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker-1-dp0656</span><span class="class">.out</span></span><br><span class="line">dp0655: starting org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker</span>, logging to /usr/install/spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>/sbin/../logs/spark-qihuang<span class="class">.zheng-org</span><span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker-1-dp0655</span><span class="class">.out</span></span><br><span class="line">dp0657: starting org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker</span>, logging to /usr/install/spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>/sbin/../logs/spark-qihuang<span class="class">.zheng-org</span><span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker-1-dp0657</span><span class="class">.out</span></span><br><span class="line">dp0652: starting org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker</span>, logging to /usr/install/spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>/sbin/../logs/spark-qihuang<span class="class">.zheng-org</span><span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker-1-dp0652</span><span class="class">.out</span></span><br><span class="line">dp0653: starting org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker</span>, logging to /usr/install/spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>/sbin/../logs/spark-qihuang<span class="class">.zheng-org</span><span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker-1-dp0653</span><span class="class">.out</span></span><br><span class="line">dp0652: starting org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker</span>, logging to /usr/install/spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>/sbin/../logs/spark-qihuang<span class="class">.zheng-org</span><span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker-2-dp0652</span><span class="class">.out</span></span><br><span class="line">dp0653: starting org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker</span>, logging to /usr/install/spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>/sbin/../logs/spark-qihuang<span class="class">.zheng-org</span><span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker-2-dp0653</span><span class="class">.out</span></span><br></pre></td></tr></table></figure>
<p>6.在master（0652）和slaves（0653,0655）上查看Spark进程。<br>可以看到master 0652上有一个Spark Master、两个Spark Worker。<br>0653上有两个Spark Worker，0655上有一个Spark Worker。</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0652 ~]$ jps -lm</span><br><span class="line"><span class="number">40708</span> org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.master</span><span class="class">.Master</span> --ip dp0652 --port <span class="number">7077</span> --webui-port <span class="number">8082</span></span><br><span class="line"><span class="number">41095</span> org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker</span> --webui-port <span class="number">8082</span> spark:<span class="comment">//dp0652:7077</span></span><br><span class="line"><span class="number">40926</span> org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker</span> --webui-port <span class="number">8081</span> spark:<span class="comment">//dp0652:7077</span></span><br><span class="line"></span><br><span class="line">[qihuang.zheng@dp0653 ~]$ jps -lm</span><br><span class="line"><span class="number">27153</span> org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker</span> --webui-port <span class="number">8082</span> spark:<span class="comment">//dp0652:7077</span></span><br><span class="line"><span class="number">27029</span> org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker</span> --webui-port <span class="number">8081</span> spark:<span class="comment">//dp0652:7077</span></span><br><span class="line"></span><br><span class="line">[qihuang.zheng@dp0655 ~]$ jps -lm</span><br><span class="line"><span class="number">8766</span> org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.worker</span><span class="class">.Worker</span> --webui-port <span class="number">8081</span> spark:<span class="comment">//dp0652:7077</span></span><br></pre></td></tr></table></figure>
<p>7.查看web ui: <a href="http://dp0652:8082/" target="_blank" rel="external">http://dp0652:8082/</a></p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/spark8.png" alt=""></p>
<p>8.<code>spark-shell</code>可以交互式地执行Spark代码，<code>spark-submit</code>则用于提交jar包。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-shell --master spark:<span class="comment">//dp0652:7077 --executor-memory 4g</span></span><br><span class="line"></span><br><span class="line">bin/spark-submit --master spark:<span class="comment">//dp0652:7077 \</span></span><br><span class="line">  --<span class="keyword">class</span> org.apache.spark.examples.SparkPi \</span><br><span class="line">  --executor-memory <span class="number">4</span>g --total-executor-cores <span class="number">2</span> \</span><br><span class="line">  lib/spark-examples-<span class="number">1.4</span><span class="number">.0</span>-hadoop2<span class="number">.6</span><span class="number">.0</span>.jar <span class="number">1000</span></span><br></pre></td></tr></table></figure>
<h3 id="问题">问题</h3><p><strong>1.如果配置了SPARK_LOCAL_IP, 但是并没有在slaves上修改为自己的IP,则会报错:</strong>  </p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">15/07/02 09:04:08 <span class="keyword">ERROR</span> netty.NettyTransport: failed to bind to /192.168.6.52:0, shutting down Netty transport</span><br><span class="line">Exception <span class="keyword">in</span> thread <span class="string">"main"</span> java.<span class="keyword">net</span>.BindException: Failed to bind to: /192.168.6.52:0: Service 'sparkWorker' failed after 16 retries!</span><br><span class="line">        at org.jboss.netty.<span class="keyword">bootstrap</span>.ServerBootstrap.bind(ServerBootstrap.java:272)</span><br><span class="line">        at akka.remote.transport.netty.NettyTransport$<span class="label">$anonfun</span><span class="label">$listen</span><span class="label">$1</span>.apply(NettyTransport.<span class="keyword">scala</span>:393)</span><br><span class="line">        at akka.remote.transport.netty.NettyTransport$<span class="label">$anonfun</span><span class="label">$listen</span><span class="label">$1</span>.apply(NettyTransport.<span class="keyword">scala</span>:389)</span><br><span class="line">        at <span class="keyword">scala</span>.util.Success$<span class="label">$anonfun</span><span class="label">$map</span><span class="label">$1</span>.apply(Try.<span class="keyword">scala</span>:206)</span><br><span class="line">        at <span class="keyword">scala</span>.util.Try$.apply(Try.<span class="keyword">scala</span>:161)</span><br><span class="line">        at <span class="keyword">scala</span>.util.Success.map(Try.<span class="keyword">scala</span>:206)</span><br><span class="line">        at <span class="keyword">scala</span>.concurrent.Future$<span class="label">$anonfun</span><span class="label">$map</span><span class="label">$1</span>.apply(Future.<span class="keyword">scala</span>:235)</span><br><span class="line">        at <span class="keyword">scala</span>.concurrent.Future$<span class="label">$anonfun</span><span class="label">$map</span><span class="label">$1</span>.apply(Future.<span class="keyword">scala</span>:235)</span><br><span class="line">        at <span class="keyword">scala</span>.concurrent.impl.CallbackRunnable.<span class="keyword">run</span>(Promise.<span class="keyword">scala</span>:32)</span><br><span class="line">        at akka.dispatch.BatchingExecutor<span class="label">$Batch</span>$<span class="label">$anonfun</span><span class="label">$run</span><span class="label">$1</span>.processBatch<span class="label">$1</span>(BatchingExecutor.<span class="keyword">scala</span>:67)</span><br><span class="line">        at akka.dispatch.BatchingExecutor<span class="label">$Batch</span>$<span class="label">$anonfun</span><span class="label">$run</span><span class="label">$1</span>.apply<span class="label">$mcV</span><span class="label">$sp</span>(BatchingExecutor.<span class="keyword">scala</span>:82)</span><br><span class="line">        at akka.dispatch.BatchingExecutor<span class="label">$Batch</span>$<span class="label">$anonfun</span><span class="label">$run</span><span class="label">$1</span>.apply(BatchingExecutor.<span class="keyword">scala</span>:59)</span><br><span class="line">        at akka.dispatch.BatchingExecutor<span class="label">$Batch</span>$<span class="label">$anonfun</span><span class="label">$run</span><span class="label">$1</span>.apply(BatchingExecutor.<span class="keyword">scala</span>:59)</span><br><span class="line">        at <span class="keyword">scala</span>.concurrent.BlockContext$.withBlockContext(BlockContext.<span class="keyword">scala</span>:72)</span><br><span class="line">        at akka.dispatch.BatchingExecutor<span class="label">$Batch</span>.<span class="keyword">run</span>(BatchingExecutor.<span class="keyword">scala</span>:58)</span><br><span class="line">        at akka.dispatch.TaskInvocation.<span class="keyword">run</span>(AbstractDispatcher.<span class="keyword">scala</span>:41)</span><br><span class="line">        at akka.dispatch.ForkJoinExecutorConfigurator<span class="label">$AkkaForkJoinTask</span>.exec(AbstractDispatcher.<span class="keyword">scala</span>:393)</span><br><span class="line">        at <span class="keyword">scala</span>.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)</span><br><span class="line">        at <span class="keyword">scala</span>.concurrent.forkjoin.ForkJoinPool<span class="label">$WorkQueue</span>.runTask(ForkJoinPool.java:1339)</span><br><span class="line">        at <span class="keyword">scala</span>.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)</span><br><span class="line">        at <span class="keyword">scala</span>.concurrent.forkjoin.ForkJoinWorkerThread.<span class="keyword">run</span>(ForkJoinWorkerThread.java:107)</span><br><span class="line">15/07/02 09:04:09 INFO remote.RemoteActorRefProvider<span class="label">$RemotingTerminator</span>: Shutting down remote daemon.</span><br><span class="line">15/07/02 09:04:09 INFO remote.RemoteActorRefProvider<span class="label">$RemotingTerminator</span>: Remote daemon shut down; proceeding with flushing remote transports.</span><br><span class="line">15/07/02 09:04:09 INFO util.Utils: Shutdown hook called</span><br></pre></td></tr></table></figure>
<p>原因分析: SPARK_LOCAL_IP指的是本机IP地址,因此分发到集群的不同节点上,都要到各自的节点修改为自己的IP地址.<br>如果集群节点比较多,则比较麻烦, 可以用SPARK_LOCAL_DIRS代替.</p>
<p><strong>2.如果没有配置export MASTER, 在worker上会报错:</strong>  </p>
<figure class="highlight autoit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">5</span>/<span class="number">07</span>/<span class="number">02</span> <span class="number">08</span>:<span class="number">40</span>:<span class="number">51</span> INFO worker.Worker: Retrying connection <span class="keyword">to</span> master (attempt <span class="preprocessor"># 12)</span></span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">02</span> <span class="number">08</span>:<span class="number">40</span>:<span class="number">51</span> INFO worker.Worker: Connecting <span class="keyword">to</span> master akka.tcp://sparkMaster<span class="constant">@dp0652</span>:<span class="number">7077</span>/user/Master...</span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">02</span> <span class="number">08</span>:<span class="number">40</span>:<span class="number">51</span> WARN Remoting: Tried <span class="keyword">to</span> associate <span class="keyword">with</span> unreachable remote address [akka.tcp://sparkMaster<span class="constant">@dp0652</span>:<span class="number">7077</span>].</span><br><span class="line">Address is <span class="built_in">now</span> gated <span class="keyword">for</span> <span class="number">5000</span> ms, all messages <span class="keyword">to</span> this address will be delivered <span class="keyword">to</span> dead letters.</span><br><span class="line">Reason: 拒绝连接: dp0652/<span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">7077</span></span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">02</span> <span class="number">08</span>:<span class="number">41</span>:<span class="number">23</span> ERROR worker.Worker: RECEIVED SIGNAL <span class="number">15</span>: SIGTERM</span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">02</span> <span class="number">08</span>:<span class="number">41</span>:<span class="number">23</span> INFO util.Utils: <span class="built_in">Shutdown</span> hook called</span><br></pre></td></tr></table></figure>
<p>导致的后果是虽然slaves上都启动了Worker进程(使用jps查看),但是在Master上并没有看到workers. 这时候应该查看Master上的日志.<br>master上启动成功显示的日志是spark@dp0652:7077. 而上面却显示的是sparkMaster@dp0652:7077. 所以应该手动export MASTER  </p>
<p><strong>3.最后成功启动集群, 在Master上的日志:</strong>  </p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">Spark <span class="string">Command:</span> <span class="regexp">/usr/</span>java<span class="regexp">/jdk1.7.0_51/</span>bin<span class="regexp">/java -cp /</span>usr<span class="regexp">/install/</span>spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span><span class="regexp">/sbin/</span>..<span class="regexp">/conf/</span>:<span class="regexp">/usr/</span>install<span class="regexp">/spark-1.4.0-bin-hadoop2.6/</span>lib<span class="regexp">/spark-assembly-1.4.0-hadoop2.6.0.jar:/</span>usr<span class="regexp">/install/</span>spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span><span class="regexp">/lib/</span>datanucleus-rdbms-<span class="number">3.2</span><span class="number">.9</span>.<span class="string">jar:</span><span class="regexp">/usr/</span>install<span class="regexp">/spark-1.4.0-bin-hadoop2.6/</span>lib<span class="regexp">/datanucleus-core-3.2.10.jar:/</span>usr<span class="regexp">/install/</span>spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span><span class="regexp">/lib/</span>datanucleus-api-jdo-<span class="number">3.2</span><span class="number">.6</span>.<span class="string">jar:</span><span class="regexp">/usr/</span>install<span class="regexp">/hadoop/</span>etc<span class="regexp">/hadoop/</span>:<span class="regexp">/usr/</span>install<span class="regexp">/hadoop/</span>etc<span class="regexp">/hadoop/</span> -Xms512m -Xmx512m -<span class="string">XX:</span>MaxPermSize=<span class="number">128</span>m org.apache.spark.deploy.master.Master --ip dp0652 --port <span class="number">7077</span> --webui-port <span class="number">8082</span></span><br><span class="line">========================================</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">49</span> INFO master.<span class="string">Master:</span> Registered signal handlers <span class="keyword">for</span> [TERM, HUP, INT]</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">50</span> WARN util.<span class="string">NativeCodeLoader:</span> Unable to load native-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes where applicable</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">50</span> INFO spark.<span class="string">SecurityManager:</span> Changing view acls <span class="string">to:</span> qihuang.zheng</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">50</span> INFO spark.<span class="string">SecurityManager:</span> Changing modify acls <span class="string">to:</span> qihuang.zheng</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">50</span> INFO spark.<span class="string">SecurityManager:</span> <span class="string">SecurityManager:</span> authentication disabled; ui acls disabled; users with view <span class="string">permissions:</span> Set(qihuang.zheng); users with modify <span class="string">permissions:</span> Set(qihuang.zheng)</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO slf4j.<span class="string">Slf4jLogger:</span> Slf4jLogger started</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO <span class="string">Remoting:</span> Starting remoting</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO <span class="string">Remoting:</span> Remoting started; listening on <span class="string">addresses :</span>[akka.<span class="string">tcp:</span><span class="comment">//sparkMaster@dp0652:7077]</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO util.<span class="string">Utils:</span> Successfully started service <span class="string">'sparkMaster'</span> on port <span class="number">7077.</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO server.<span class="string">Server:</span> jetty-<span class="number">8.</span>y.z-SNAPSHOT</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO server.<span class="string">AbstractConnector:</span> Started SelectChannelConnector<span class="annotation">@dp</span><span class="number">0652:</span><span class="number">6066</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO util.<span class="string">Utils:</span> Successfully started service on port <span class="number">6066.</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO rest.<span class="string">StandaloneRestServer:</span> Started REST server <span class="keyword">for</span> submitting applications on port <span class="number">6066</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO master.<span class="string">Master:</span> Starting Spark master at <span class="string">spark:</span><span class="comment">//dp0652:7077</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO master.<span class="string">Master:</span> Running Spark version <span class="number">1.4</span><span class="number">.0</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO server.<span class="string">Server:</span> jetty-<span class="number">8.</span>y.z-SNAPSHOT</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO server.<span class="string">AbstractConnector:</span> Started SelectChannelConnector@<span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span>:<span class="number">8082</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO util.<span class="string">Utils:</span> Successfully started service <span class="string">'MasterUI'</span> on port <span class="number">8082.</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">51</span> INFO ui.<span class="string">MasterWebUI:</span> Started MasterWebUI at <span class="string">http:</span><span class="comment">//192.168.6.52:8082</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">52</span> INFO master.<span class="string">Master:</span> I have been elected leader! New <span class="string">state:</span> ALIVE</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO master.<span class="string">Master:</span> Registering worker <span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">35398</span> with <span class="number">1</span> cores, <span class="number">20.0</span> GB RAM</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO master.<span class="string">Master:</span> Registering worker <span class="number">192.168</span><span class="number">.6</span><span class="number">.56</span>:<span class="number">60106</span> with <span class="number">1</span> cores, <span class="number">8.0</span> GB RAM</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO master.<span class="string">Master:</span> Registering worker <span class="number">192.168</span><span class="number">.6</span><span class="number">.55</span>:<span class="number">50995</span> with <span class="number">1</span> cores, <span class="number">8.0</span> GB RAM</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO master.<span class="string">Master:</span> Registering worker <span class="number">192.168</span><span class="number">.6</span><span class="number">.53</span>:<span class="number">55994</span> with <span class="number">1</span> cores, <span class="number">20.0</span> GB RAM</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO master.<span class="string">Master:</span> Registering worker <span class="number">192.168</span><span class="number">.6</span><span class="number">.57</span>:<span class="number">34020</span> with <span class="number">1</span> cores, <span class="number">8.0</span> GB RAM</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">56</span> INFO master.<span class="string">Master:</span> Registering worker <span class="number">192.168</span><span class="number">.6</span><span class="number">.52</span>:<span class="number">55912</span> with <span class="number">1</span> cores, <span class="number">20.0</span> GB RAM</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">56</span> INFO master.<span class="string">Master:</span> Registering worker <span class="number">192.168</span><span class="number">.6</span><span class="number">.53</span>:<span class="number">35846</span> with <span class="number">1</span> cores, <span class="number">20.0</span> GB RAM</span><br></pre></td></tr></table></figure>
<p><strong>在53的其中一个Worker上的日志:</strong>  </p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Spark <span class="string">Command:</span> <span class="regexp">/usr/</span>java<span class="regexp">/jdk1.7.0_51/</span>bin<span class="regexp">/java -cp /</span>usr<span class="regexp">/install/</span>spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span><span class="regexp">/sbin/</span>..<span class="regexp">/conf/</span>:<span class="regexp">/usr/</span>install<span class="regexp">/spark-1.4.0-bin-hadoop2.6/</span>lib<span class="regexp">/spark-assembly-1.4.0-hadoop2.6.0.jar:/</span>usr<span class="regexp">/install/</span>spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span><span class="regexp">/lib/</span>datanucleus-api-jdo-<span class="number">3.2</span><span class="number">.6</span>.<span class="string">jar:</span><span class="regexp">/usr/</span>install<span class="regexp">/spark-1.4.0-bin-hadoop2.6/</span>lib<span class="regexp">/datanucleus-core-3.2.10.jar:/</span>usr<span class="regexp">/install/</span>spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span><span class="regexp">/lib/</span>datanucleus-rdbms-<span class="number">3.2</span><span class="number">.9</span>.<span class="string">jar:</span><span class="regexp">/usr/</span>install<span class="regexp">/hadoop/</span>etc<span class="regexp">/hadoop/</span>:<span class="regexp">/usr/</span>install<span class="regexp">/hadoop/</span>etc<span class="regexp">/hadoop/</span> -Xms512m -Xmx512m -<span class="string">XX:</span>MaxPermSize=<span class="number">128</span>m org.apache.spark.deploy.worker.Worker --webui-port <span class="number">8081</span> <span class="string">spark:</span><span class="comment">//dp0652:7077</span></span><br><span class="line">========================================</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">52</span> INFO worker.<span class="string">Worker:</span> Registered signal handlers <span class="keyword">for</span> [TERM, HUP, INT]</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">52</span> WARN util.<span class="string">NativeCodeLoader:</span> Unable to load native-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes where applicable</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">52</span> INFO spark.<span class="string">SecurityManager:</span> Changing view acls <span class="string">to:</span> qihuang.zheng</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">52</span> INFO spark.<span class="string">SecurityManager:</span> Changing modify acls <span class="string">to:</span> qihuang.zheng</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">52</span> INFO spark.<span class="string">SecurityManager:</span> <span class="string">SecurityManager:</span> authentication disabled; ui acls disabled; users with view <span class="string">permissions:</span> Set(qihuang.zheng); users with modify <span class="string">permissions:</span> Set(qihuang.zheng)</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">53</span> INFO slf4j.<span class="string">Slf4jLogger:</span> Slf4jLogger started</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">53</span> INFO <span class="string">Remoting:</span> Starting remoting</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO <span class="string">Remoting:</span> Remoting started; listening on <span class="string">addresses :</span>[akka.<span class="string">tcp:</span><span class="comment">//sparkWorker@192.168.6.53:55994]</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO util.<span class="string">Utils:</span> Successfully started service <span class="string">'sparkWorker'</span> on port <span class="number">55994.</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO worker.<span class="string">Worker:</span> Starting Spark worker <span class="number">192.168</span><span class="number">.6</span><span class="number">.53</span>:<span class="number">55994</span> with <span class="number">1</span> cores, <span class="number">20.0</span> GB RAM</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO worker.<span class="string">Worker:</span> Running Spark version <span class="number">1.4</span><span class="number">.0</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO worker.<span class="string">Worker:</span> Spark <span class="string">home:</span> <span class="regexp">/usr/</span>install/spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO server.<span class="string">Server:</span> jetty-<span class="number">8.</span>y.z-SNAPSHOT</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO server.<span class="string">AbstractConnector:</span> Started SelectChannelConnector@<span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span>:<span class="number">8081</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO util.<span class="string">Utils:</span> Successfully started service <span class="string">'WorkerUI'</span> on port <span class="number">8081.</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO ui.<span class="string">WorkerWebUI:</span> Started WorkerWebUI at <span class="string">http:</span><span class="comment">//192.168.6.53:8081</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO worker.<span class="string">Worker:</span> Connecting to master akka.<span class="string">tcp:</span><span class="comment">//sparkMaster@dp0652:7077/user/Master...</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">02</span> <span class="number">09</span>:<span class="number">27</span>:<span class="number">54</span> INFO worker.<span class="string">Worker:</span> Successfully registered with master <span class="string">spark:</span><span class="comment">//dp0652:7077</span></span><br></pre></td></tr></table></figure>
<h2 id="Spark_Standalone_HA（30分钟）">Spark Standalone HA（30分钟）</h2><p>参考文档：</p>
<ul>
<li><a href="http://taoistwar.gitbooks.io/spark-operationand-maintenance-management/content/spark_install/spark_standalone_with_zookeeper_ha.html" target="_blank" rel="external">Spark Standalone基于ZooKeeper的HA</a></li>
<li><a href="http://www.cnblogs.com/byrhuangqiang/p/3937654.html" target="_blank" rel="external">Spark:Master High Availability（HA）高可用配置的2种实现</a></li>
</ul>
<p>简单问答：</p>
<ul>
<li>为什么需要HA？因为Master只有一个节点，会出现单点故障。如果Master挂掉了，Spark集群就不可用。</li>
<li>怎么实现HA？使用ZooKeeper，启动两个Master，只有一个Master会起作用，另一个是Standby。</li>
</ul>
<p>以前面的五台机器为例，实现HA的部署结构：</p>
<table>
<thead>
<tr>
<th>node</th>
<th>host</th>
</tr>
</thead>
<tbody>
<tr>
<td>masters</td>
<td>dp0652,dp0653</td>
</tr>
<tr>
<td>slaves</td>
<td>dp0655,dp0656,dp0657</td>
</tr>
</tbody>
</table>
<p>1.修改配置文件：</p>
<figure class="highlight objectivec"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ vi spark-env<span class="variable">.sh</span></span><br><span class="line"><span class="keyword">export</span> SPARK_MASTER_IP=dp0652</span><br><span class="line"><span class="keyword">export</span> SPARK_DAEMON_J<span class="built_in">AVA_OPTS</span>=<span class="string">"-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=dp0655:2181,dp0656:2181,dp0657:2181 -Dspark.deploy.zookeeper.dir=/spark"</span></span><br></pre></td></tr></table></figure>
<p>2.修改slaves</p>
<figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$ </span>vi slaves</span><br><span class="line">dp0655</span><br><span class="line">dp0656</span><br><span class="line">dp0657</span><br></pre></td></tr></table></figure>
<p>3.将配置文件分发到集群所有节点</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scp spark-env.sh dp0653:/usr/install/spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span>/conf</span><br><span class="line">scp spark-env.sh dp0655:/usr/install/spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span>/conf</span><br><span class="line">scp spark-env.sh dp0656:/usr/install/spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span>/conf</span><br><span class="line">scp spark-env.sh dp0657:/usr/install/spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span>/conf</span><br><span class="line">scp slaves dp0653:/usr/install/spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span>/conf</span><br><span class="line">scp slaves dp0655:/usr/install/spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span>/conf</span><br><span class="line">scp slaves dp0656:/usr/install/spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span>/conf</span><br><span class="line">scp slaves dp0657:/usr/install/spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span>/conf</span><br></pre></td></tr></table></figure>
<p>4.在dp0653上修改spark-env.sh</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SPARK_MASTER_IP=dp0653</span><br></pre></td></tr></table></figure>
<p>5.在dp0652上启动集群: <code>sbin/start-all.sh</code></p>
<p>6.在dp0653上启动Master: <code>sbin/start-master.sh</code></p>
<p>7.可以看到dp0652是active, dp0653是standby</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/sha1.png" alt=""></p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/sha2.png" alt=""></p>
<p>8.关闭dp0652的master: <code>sbin/stop-master.sh</code></p>
<p>9.观察dp0653是否成为master:</p>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/sha3.png" alt=""></p>
<p>10.执行应用程序. 注意–master现在有多个</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit --master spark:<span class="comment">//dp0652:7077,dp0653:7077 \</span></span><br><span class="line">  --<span class="keyword">class</span> org.apache.spark.examples.SparkPi \</span><br><span class="line">  --executor-memory <span class="number">4</span>g --total-executor-cores <span class="number">2</span> \</span><br><span class="line">  lib/spark-examples-<span class="number">1.4</span><span class="number">.0</span>-hadoop2<span class="number">.6</span><span class="number">.0</span>.jar <span class="number">10</span></span><br></pre></td></tr></table></figure>
<p>10.观察dp0652的master日志:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">07</span> <span class="number">14</span>:<span class="number">41</span>:<span class="number">50</span> INFO Master: Registering worker <span class="number">192.168</span><span class="number">.6</span><span class="number">.55</span>:<span class="number">58543</span> with <span class="number">1</span> cores, <span class="number">8.0</span> GB RAM</span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">07</span> <span class="number">14</span>:<span class="number">41</span>:<span class="number">50</span> INFO Master: Registering worker <span class="number">192.168</span><span class="number">.6</span><span class="number">.56</span>:<span class="number">37859</span> with <span class="number">1</span> cores, <span class="number">8.0</span> GB RAM</span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">07</span> <span class="number">14</span>:<span class="number">41</span>:<span class="number">50</span> INFO Master: Registering worker <span class="number">192.168</span><span class="number">.6</span><span class="number">.57</span>:<span class="number">34379</span> with <span class="number">1</span> cores, <span class="number">8.0</span> GB RAM</span><br><span class="line"></span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">07</span> <span class="number">14</span>:<span class="number">45</span>:<span class="number">01</span> ERROR Master: RECEIVED SIGNAL <span class="number">15</span>: SIGTERM</span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">07</span> <span class="number">14</span>:<span class="number">45</span>:<span class="number">01</span> INFO Utils: Shutdown hook called</span><br></pre></td></tr></table></figure>
<p>11.观察dp0653的master日志:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">07</span> <span class="number">14</span>:<span class="number">42</span>:<span class="number">03</span> INFO ConnectionStateManager: State change: CONNECTED</span><br><span class="line"></span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">07</span> <span class="number">14</span>:<span class="number">45</span>:<span class="number">35</span> INFO ZooKeeperLeaderElectionAgent: We have gained leadership</span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">07</span> <span class="number">14</span>:<span class="number">45</span>:<span class="number">36</span> INFO Master: I have been elected leader! New state: RECOVERING</span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">07</span> <span class="number">14</span>:<span class="number">45</span>:<span class="number">36</span> INFO Master: Trying to recover worker: worker-<span class="number">20150707144149</span>-<span class="number">192.168</span><span class="number">.6</span><span class="number">.56</span>-<span class="number">37859</span></span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">07</span> <span class="number">14</span>:<span class="number">45</span>:<span class="number">36</span> INFO Master: Trying to recover worker: worker-<span class="number">20150707144149</span>-<span class="number">192.168</span><span class="number">.6</span><span class="number">.55</span>-<span class="number">58543</span></span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">07</span> <span class="number">14</span>:<span class="number">45</span>:<span class="number">36</span> INFO Master: Trying to recover worker: worker-<span class="number">20150707144149</span>-<span class="number">192.168</span><span class="number">.6</span><span class="number">.57</span>-<span class="number">34379</span></span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">07</span> <span class="number">14</span>:<span class="number">45</span>:<span class="number">36</span> INFO Master: Worker has been re-registered: worker-<span class="number">20150707144149</span>-<span class="number">192.168</span><span class="number">.6</span><span class="number">.55</span>-<span class="number">58543</span></span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">07</span> <span class="number">14</span>:<span class="number">45</span>:<span class="number">36</span> INFO Master: Worker has been re-registered: worker-<span class="number">20150707144149</span>-<span class="number">192.168</span><span class="number">.6</span><span class="number">.56</span>-<span class="number">37859</span></span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">07</span> <span class="number">14</span>:<span class="number">45</span>:<span class="number">36</span> INFO Master: Worker has been re-registered: worker-<span class="number">20150707144149</span>-<span class="number">192.168</span><span class="number">.6</span><span class="number">.57</span>-<span class="number">34379</span></span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">07</span> <span class="number">14</span>:<span class="number">45</span>:<span class="number">36</span> INFO Master: Recovery complete - resuming operations!</span><br></pre></td></tr></table></figure>
<h2 id="Spark_SQL">Spark SQL</h2><h3 id="Hive_on_Spark">Hive on Spark</h3><p>编译支持hive的spark</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mvn -Pyarn -Dyarn.version=<span class="number">2.6</span><span class="number">.0</span> -Phadoop-<span class="number">2.6</span> -Dhadoop.version=<span class="number">2.6</span><span class="number">.0</span> \</span><br><span class="line">-Phive -Phive-<span class="number">0.13</span><span class="number">.1</span> -Phive-thriftserver \</span><br><span class="line">-DskipTests clean package</span><br></pre></td></tr></table></figure>
<p>如果没有编译hive on spark,而是直接把hive-site.xml分发到spark集群的conf目录下,直接启动spark-sql会报错:</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0652 spark-<span class="number">1.4</span>.<span class="number">0</span>-bin-hadoop2.<span class="number">6</span>]$ bin/spark-sql</span><br><span class="line">Exception <span class="keyword">in</span> thread <span class="string">"main"</span> java<span class="class">.lang</span><span class="class">.RuntimeException</span>: java<span class="class">.io</span><span class="class">.IOException</span>: 权限不够</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.hadoop</span><span class="class">.hive</span><span class="class">.ql</span><span class="class">.session</span><span class="class">.SessionState</span><span class="class">.start</span>(SessionState<span class="class">.java</span>:<span class="number">330</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.sql</span><span class="class">.hive</span><span class="class">.thriftserver</span><span class="class">.SparkSQLCLIDriver</span>$.<span class="function"><span class="title">main</span><span class="params">(SparkSQLCLIDriver.scala:<span class="number">109</span>)</span></span></span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.sql</span><span class="class">.hive</span><span class="class">.thriftserver</span><span class="class">.SparkSQLCLIDriver</span><span class="class">.main</span>(SparkSQLCLIDriver.scala)</span><br><span class="line">    at sun<span class="class">.reflect</span><span class="class">.NativeMethodAccessorImpl</span><span class="class">.invoke0</span>(Native Method)</span><br><span class="line">    at sun<span class="class">.reflect</span><span class="class">.NativeMethodAccessorImpl</span><span class="class">.invoke</span>(NativeMethodAccessorImpl<span class="class">.java</span>:<span class="number">57</span>)</span><br><span class="line">    at sun<span class="class">.reflect</span><span class="class">.DelegatingMethodAccessorImpl</span><span class="class">.invoke</span>(DelegatingMethodAccessorImpl<span class="class">.java</span>:<span class="number">43</span>)</span><br><span class="line">    at java<span class="class">.lang</span><span class="class">.reflect</span><span class="class">.Method</span><span class="class">.invoke</span>(Method<span class="class">.java</span>:<span class="number">606</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.SparkSubmit</span>$.org<span class="variable">$apache</span><span class="variable">$spark</span><span class="variable">$deploy</span><span class="variable">$SparkSubmit</span>$<span class="variable">$runMain</span>(SparkSubmit<span class="class">.scala</span>:<span class="number">664</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.SparkSubmit</span>$.doRunMain$<span class="number">1</span>(SparkSubmit<span class="class">.scala</span>:<span class="number">169</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.SparkSubmit</span>$.<span class="function"><span class="title">submit</span><span class="params">(SparkSubmit.scala:<span class="number">192</span>)</span></span></span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.SparkSubmit</span>$.<span class="function"><span class="title">main</span><span class="params">(SparkSubmit.scala:<span class="number">111</span>)</span></span></span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.deploy</span><span class="class">.SparkSubmit</span><span class="class">.main</span>(SparkSubmit.scala)</span><br><span class="line">Caused by: java<span class="class">.io</span><span class="class">.IOException</span>: 权限不够</span><br><span class="line">    at java<span class="class">.io</span><span class="class">.UnixFileSystem</span><span class="class">.createFileExclusively</span>(Native Method)</span><br><span class="line">    at java<span class="class">.io</span><span class="class">.File</span><span class="class">.createNewFile</span>(File<span class="class">.java</span>:<span class="number">1006</span>)</span><br><span class="line">    at java<span class="class">.io</span><span class="class">.File</span><span class="class">.createTempFile</span>(File<span class="class">.java</span>:<span class="number">1989</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.hadoop</span><span class="class">.hive</span><span class="class">.ql</span><span class="class">.session</span><span class="class">.SessionState</span><span class="class">.createTempFile</span>(SessionState<span class="class">.java</span>:<span class="number">432</span>)</span><br><span class="line">    at org<span class="class">.apache</span><span class="class">.hadoop</span><span class="class">.hive</span><span class="class">.ql</span><span class="class">.session</span><span class="class">.SessionState</span><span class="class">.start</span>(SessionState<span class="class">.java</span>:<span class="number">328</span>)</span><br><span class="line">    ... <span class="number">11</span> more</span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">03</span> <span class="number">08</span>:<span class="number">42</span>:<span class="number">33</span> INFO util<span class="class">.Utils</span>: Shutdown hook called</span><br><span class="line"><span class="number">15</span>/<span class="number">07</span>/<span class="number">03</span> <span class="number">08</span>:<span class="number">42</span>:<span class="number">33</span> INFO util<span class="class">.Utils</span>: Deleting directory /tmp/spark-<span class="number">831</span>ff199-cf80-<span class="number">4</span>d49-a22f-<span class="number">824736065289</span></span><br></pre></td></tr></table></figure>
<p>这是因为Spark集群的每个Worker都需要Hive的支持,而Worker节点并没有都安装了hive. 而且spark需要编译支持hive的包.<br>但是重新编译hive on spark要花很多时间,可不可以直接使用集群中已经安装好的hive呢?  YES!!<br><a href="http://lxw1234.com/archives/2015/06/294.htm" target="_blank" rel="external">http://lxw1234.com/archives/2015/06/294.htm</a><br><a href="http://shiyanjun.cn/archives/1113.html" target="_blank" rel="external">http://shiyanjun.cn/archives/1113.html</a><br><a href="http://www.cnblogs.com/hseagle/p/3758922.html" target="_blank" rel="external">http://www.cnblogs.com/hseagle/p/3758922.html</a>  </p>
<p>1.在spark-env.sh中添加</p>
<figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HIVE_HOME=/usr/install/apache-hive-<span class="number">0</span>.<span class="number">13.1</span>-bin</span><br><span class="line">export SPARK_CLASSPATH=<span class="variable">$HIVE</span>_HOME/lib/mysql-connector-java-<span class="number">5.1</span>.<span class="number">34</span>.jar:<span class="variable">$SPARK</span>_CLASSPATH</span><br></pre></td></tr></table></figure>
<p>2.将<code>apache-hive-0.13.1-bin</code>分发到集群中的每个节点(SparkWorker所在的节点)</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd <span class="operator"><span class="keyword">install</span></span><br><span class="line">scp -r apache-hive-<span class="number">0.13</span><span class="number">.1</span>-<span class="keyword">bin</span> dp0653:/usr/<span class="keyword">install</span>/</span></span><br></pre></td></tr></table></figure>
<p>3.拷贝apache-hive-0.13.1-bin/conf/hive-site.xml到$SPARK_HOME/conf下</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp apache-hive-<span class="number">0.13</span><span class="number">.1</span>-bin/conf/hive-site.xml dp0653:/usr/install/spark-<span class="number">1.4</span><span class="number">.0</span>-bin-hadoop2<span class="number">.6</span>/conf</span><br></pre></td></tr></table></figure>
<p>4.重启spark集群(standalone模式)</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sbin/<span class="keyword">stop</span>-<span class="keyword">all</span>.<span class="keyword">sh</span></span><br><span class="line">sbin/start-<span class="keyword">all</span>.<span class="keyword">sh</span></span><br></pre></td></tr></table></figure>
<p>5.测试spark-sql</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">03</span> <span class="number">10</span>:<span class="number">00</span>:<span class="number">56</span> WARN spark.<span class="string">SparkConf:</span> Setting <span class="string">'spark.executor.extraClassPath'</span> to <span class="string">'/usr/install/apache-hive-0.13.1-bin/lib/mysql-connector-java-5.1.34.jar:'</span> <span class="keyword">as</span> a work-around.</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">03</span> <span class="number">10</span>:<span class="number">00</span>:<span class="number">56</span> WARN spark.<span class="string">SparkConf:</span> Setting <span class="string">'spark.driver.extraClassPath'</span> to <span class="string">'/usr/install/apache-hive-0.13.1-bin/lib/mysql-connector-java-5.1.34.jar:'</span> <span class="keyword">as</span> a work-around.</span><br><span class="line"></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">03</span> <span class="number">10</span>:<span class="number">01</span>:<span class="number">00</span> INFO hive.<span class="string">metastore:</span> Trying to connect to metastore with URI <span class="string">thrift:</span><span class="comment">//192.168.6.53:9083</span></span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">03</span> <span class="number">10</span>:<span class="number">01</span>:<span class="number">00</span> INFO hive.<span class="string">metastore:</span> Connected to metastore.</span><br><span class="line"><span class="number">15</span><span class="regexp">/07/</span><span class="number">03</span> <span class="number">10</span>:<span class="number">01</span>:<span class="number">00</span> INFO session.<span class="string">SessionState:</span> No Tez session required at <span class="keyword">this</span> point. hive.execution.engine=mr.</span><br><span class="line">SET spark.sql.hive.version=<span class="number">0.13</span><span class="number">.1</span></span><br><span class="line">SET spark.sql.hive.version=<span class="number">0.13</span><span class="number">.1</span></span><br><span class="line"></span><br><span class="line">spark-sql&gt; show databases;</span><br><span class="line"><span class="keyword">default</span></span><br><span class="line">test</span><br><span class="line">spark-sql&gt; use test;</span><br><span class="line">spark-sql&gt; show tables;</span><br><span class="line">koudai  <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">spark-sql&gt; select count(*) from koudai;</span><br><span class="line"><span class="number">311839</span></span><br></pre></td></tr></table></figure>
<p><img src="http://7xjs7x.com1.z0.glb.clouddn.com/spark-perf.png" alt=""></p>
<h3 id="SparkSQL_&amp;_thrift">SparkSQL &amp; thrift</h3><p>直接用bin/spark-sql启动:  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0653 ~]$ jps -lm</span><br><span class="line"><span class="number">35146</span> org.apache.spark.deploy.SparkSubmit --master spark:<span class="comment">//192.168.6.52:7078 --class org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver spark-internal</span></span><br><span class="line"><span class="number">35668</span> sun.tools.jps.Jps -lm</span><br><span class="line">[qihuang.zheng@dp0653 ~]$ ps -ef | grep SparkSQL</span><br><span class="line"><span class="number">506</span>      <span class="number">35146</span> <span class="number">35011</span> <span class="number">26</span> <span class="number">09</span>:<span class="number">16</span> pts/<span class="number">15</span>   <span class="number">00</span>:<span class="number">00</span>:<span class="number">19</span> /usr/java/jdk1<span class="number">.7</span><span class="number">.0</span>_51/bin/java ... org.apache.spark.deploy.SparkSubmit --master spark:<span class="comment">//192.168.6.52:7078 --class org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver spark-internal</span></span><br></pre></td></tr></table></figure>
<p>hive在53上, 查看thrift服务:  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0653 ~]$ ps -ef | grep thrift</span><br><span class="line">admin    <span class="number">28541</span>     <span class="number">1</span>  <span class="number">0</span> Aug19 ?        <span class="number">00</span>:<span class="number">02</span>:<span class="number">47</span> /usr/java/jdk1<span class="number">.7</span><span class="number">.0</span>_51/bin/java ... org.apache.hadoop.util.RunJar /usr/install/apache-hive-<span class="number">1.2</span><span class="number">.0</span>-bin/lib/hive-service-<span class="number">1.2</span><span class="number">.0</span>.jar org.apache.hive.service.server.HiveServer2 --hiveconf hive.metastore.uris=thrift:<span class="comment">//192.168.6.53:9083 --hiveconf hive.metastore.local=false --hiveconf hive.server2.thrift.bind.host=192.168.6.53 --hiveconf hive.server2.thrift.port=10001</span></span><br></pre></td></tr></table></figure>
<p>启动spark的thrift server:  </p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo sbin/start-thriftserver<span class="class">.sh</span> --master spark:<span class="comment">//192.168.6.52:7078</span></span><br><span class="line">starting org<span class="class">.apache</span><span class="class">.spark</span><span class="class">.sql</span><span class="class">.hive</span><span class="class">.thriftserver</span><span class="class">.HiveThriftServer2</span>, logging to /usr/install/spark-<span class="number">1.4</span>.<span class="number">1</span>-bin-hadoop2.<span class="number">6</span>/sbin/../logs/spark-qihuang<span class="class">.zheng-org</span><span class="class">.apache</span><span class="class">.spark</span><span class="class">.sql</span><span class="class">.hive</span><span class="class">.thriftserver</span><span class="class">.HiveThriftServer2-1-dp0652</span><span class="class">.out</span></span><br></pre></td></tr></table></figure>
<p>在启动thrift-server的时候, 指定master, 会在master的web ui上看到app. 但是启动完成后, app就结束了.<br>根据日志信息, 由于没有正确指定端口,导致无法连接   </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">15</span>/<span class="number">08</span>/<span class="number">21</span> <span class="number">09</span>:<span class="number">48</span>:<span class="number">39</span> INFO hive.metastore: Trying to connect to metastore with URI thrift:<span class="comment">//192.168.6.53:9083</span></span><br><span class="line"><span class="number">15</span>/<span class="number">08</span>/<span class="number">21</span> <span class="number">09</span>:<span class="number">48</span>:<span class="number">39</span> INFO hive.metastore: Connected to metastore.</span><br><span class="line"><span class="number">15</span>/<span class="number">08</span>/<span class="number">21</span> <span class="number">09</span>:<span class="number">48</span>:<span class="number">39</span> INFO service.AbstractService: Service:ThriftBinaryCLIService is started.</span><br><span class="line"><span class="number">15</span>/<span class="number">08</span>/<span class="number">21</span> <span class="number">09</span>:<span class="number">48</span>:<span class="number">39</span> INFO service.AbstractService: Service:HiveServer2 is started.</span><br><span class="line"><span class="number">15</span>/<span class="number">08</span>/<span class="number">21</span> <span class="number">09</span>:<span class="number">48</span>:<span class="number">39</span> INFO thriftserver.HiveThriftServer2: HiveThriftServer2 started</span><br><span class="line"><span class="number">15</span>/<span class="number">08</span>/<span class="number">21</span> <span class="number">09</span>:<span class="number">48</span>:<span class="number">39</span> WARN conf.HiveConf: DEPRECATED: hive.metastore.ds.retry.* no longer has any effect.  Use hive.hmshandler.retry.* instead</span><br><span class="line"><span class="number">15</span>/<span class="number">08</span>/<span class="number">21</span> <span class="number">09</span>:<span class="number">48</span>:<span class="number">39</span> ERROR thrift.ThriftCLIService: Error:</span><br><span class="line">org.apache.thrift.transport.TTransportException: Could not create ServerSocket on address /<span class="number">192.168</span><span class="number">.47</span><span class="number">.213</span>:<span class="number">10000.</span></span><br><span class="line">        at org.apache.thrift.transport.TServerSocket.&lt;init&gt;(TServerSocket.java:<span class="number">93</span>)</span><br><span class="line">        at org.apache.thrift.transport.TServerSocket.&lt;init&gt;(TServerSocket.java:<span class="number">79</span>)</span><br><span class="line">        at org.apache.hive.service.auth.HiveAuthFactory.getServerSocket(HiveAuthFactory.java:<span class="number">236</span>)</span><br><span class="line">        at org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.run(ThriftBinaryCLIService.java:<span class="number">69</span>)</span><br><span class="line">        at java.lang.Thread.run(Thread.java:<span class="number">744</span>)</span><br></pre></td></tr></table></figure>
<p>指定hive的端口:   </p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo sbin/start-thriftserver<span class="class">.sh</span> \</span><br><span class="line">  --hiveconf hive<span class="class">.server2</span><span class="class">.thrift</span><span class="class">.port</span>=<span class="number">10001</span> \</span><br><span class="line">  --hiveconf hive<span class="class">.server2</span><span class="class">.thrift</span><span class="class">.bind</span><span class="class">.host</span>=<span class="number">192.168</span>.<span class="number">6.53</span> \</span><br><span class="line">  --master spark:<span class="comment">//192.168.6.52:7078</span></span><br></pre></td></tr></table></figure>
<p>查看thrift进程:  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0652 spark-<span class="number">1.4</span><span class="number">.1</span>-bin-hadoop2<span class="number">.6</span>]$ ps -ef | grep thrift</span><br><span class="line">root     <span class="number">24997</span>     <span class="number">1</span> <span class="number">99</span> <span class="number">09</span>:<span class="number">55</span> pts/<span class="number">0</span>    <span class="number">00</span>:<span class="number">00</span>:<span class="number">12</span> /usr/java/jdk1<span class="number">.7</span><span class="number">.0</span>_51/bin/java .. org.apache.spark.deploy.SparkSubmit --master spark:<span class="comment">//192.168.6.52:7078 --class org.apache.spark.sql.hive.thriftserver.HiveThriftServer2 spark-internal --hiveconf hive.server2.thrift.port=10001 --hiveconf hive.server2.thrift.bind.host=192.168.6.53</span></span><br></pre></td></tr></table></figure>
<p>但是日志还是报错:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">15</span>/<span class="number">08</span>/<span class="number">21</span> <span class="number">09</span>:<span class="number">55</span>:<span class="number">42</span> ERROR thrift.ThriftCLIService: Error:</span><br><span class="line">org.apache.thrift.transport.TTransportException: Could not create ServerSocket on address /<span class="number">192.168</span><span class="number">.6</span><span class="number">.53</span>:<span class="number">10001.</span></span><br></pre></td></tr></table></figure>
<p>过了几秒,再次查看thrift进程, 找不到HiveThriftServer2了!</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">qihuang.zheng<span class="annotation">@dp</span>0652 spark-<span class="number">1.4</span><span class="number">.1</span>-bin-hadoop2<span class="number">.6</span>]$ bin<span class="regexp">/beeline -u jdbc:hive2:/</span>/<span class="number">192.168</span><span class="number">.6</span><span class="number">.53</span>:<span class="number">10001</span></span><br><span class="line">scan complete <span class="keyword">in</span> <span class="number">3</span>ms</span><br><span class="line">Connecting to <span class="string">jdbc:</span><span class="string">hive2:</span><span class="comment">//192.168.6.53:10001</span></span><br><span class="line">Connected <span class="string">to:</span> Apache Hive (version <span class="number">1.2</span><span class="number">.0</span>)</span><br><span class="line"><span class="string">Driver:</span> Spark Project Core (version <span class="number">1.4</span><span class="number">.1</span>)</span><br><span class="line">Transaction <span class="string">isolation:</span> TRANSACTION_REPEATABLE_READ</span><br><span class="line">Beeline version <span class="number">1.4</span><span class="number">.1</span> by Apache Hive</span><br><span class="line"><span class="number">0</span>: <span class="string">jdbc:</span><span class="string">hive2:</span><span class="comment">//192.168.6.53:10001&gt; show tables;</span></span><br></pre></td></tr></table></figure>
<p><a href="http://lxw1234.com/archives/2015/12/593.htm" target="_blank" rel="external">http://lxw1234.com/archives/2015/12/593.htm</a></p>
<h3 id="Spark-SQL配置参数">Spark-SQL配置参数</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">/usr/<span class="operator"><span class="keyword">install</span>/spark-<span class="number">1.4</span><span class="number">.1</span>-<span class="keyword">bin</span>-hadoop2<span class="number">.4</span>/<span class="keyword">bin</span>/spark-shell <span class="comment">--conf spark.driver.maxResultSize=2g --conf spark.executor.memory=512m                              </span></span><br><span class="line"></span><br><span class="line">/usr/<span class="keyword">install</span>/spark-<span class="number">1.4</span><span class="number">.1</span>-<span class="keyword">bin</span>-hadoop2<span class="number">.4</span>/<span class="keyword">bin</span>/spark-shell <span class="comment">--driver-java-options -Dspark.driver.maxResultSize=2g</span></span><br><span class="line">scala&gt; sc.getConf.<span class="keyword">get</span>(<span class="string">"spark.driver.maxResultSize"</span>)</span><br><span class="line">res1: <span class="keyword">String</span> = <span class="number">2</span><span class="keyword">g</span></span><br><span class="line"></span><br><span class="line">/usr/<span class="keyword">install</span>/spark-<span class="number">1.4</span><span class="number">.1</span>-<span class="keyword">bin</span>-hadoop2<span class="number">.4</span>/<span class="keyword">bin</span>/spark-<span class="keyword">sql</span> <span class="comment">--driver-java-options -Dspark.driver.maxResultSize=2g</span></span><br><span class="line">/usr/<span class="keyword">install</span>/spark-<span class="number">1.4</span><span class="number">.1</span>-<span class="keyword">bin</span>-hadoop2<span class="number">.4</span>/<span class="keyword">bin</span>/spark-<span class="keyword">sql</span> <span class="comment">--conf spark.driver.maxResultSize=2g</span></span><br><span class="line"></span><br><span class="line">/usr/<span class="keyword">install</span>/spark-<span class="number">1.4</span><span class="number">.1</span>-<span class="keyword">bin</span>-hadoop2<span class="number">.4</span>/<span class="keyword">bin</span>/spark-<span class="keyword">sql</span> -h</span><br><span class="line"><span class="keyword">Usage</span>: ./<span class="keyword">bin</span>/spark-<span class="keyword">sql</span> [options] [cli <span class="keyword">option</span>]</span><br><span class="line">  <span class="comment">--driver-java-options       Extra Java options to pass to the driver.</span></span><br><span class="line">  <span class="comment">--conf PROP=VALUE           Arbitrary Spark configuration property.</span></span><br><span class="line">  <span class="comment">--properties-file FILE      Path to a file from which to load extra properties. If not</span></span><br><span class="line">                              specified, this will look <span class="keyword">for</span> conf/spark-<span class="keyword">defaults</span>.conf.</span></span><br></pre></td></tr></table></figure>

      
    </div>
    
  </div>
  
    
<div class="copyright">
  <p><span>本文标题:</span><a href="/2015/06/20/2015-06-20-Apache-Spark/">Apache Spark入门</a></p>
  <p><span>文章作者:</span><a href="/" title="访问 任何忧伤,都抵不过世界的美丽 的个人博客">任何忧伤,都抵不过世界的美丽</a></p>
  <p><span>发布时间:</span>2015年06月20日 - 00时00分</p>
  <p><span>最后更新:</span>2017年07月17日 - 09时23分</p>
  <p>
    <span>原始链接:</span><a href="/2015/06/20/2015-06-20-Apache-Spark/" title="Apache Spark入门">http://github.com/zqhxuyuan/2015/06/20/2015-06-20-Apache-Spark/</a>
    <span class="btn" data-clipboard-text="原文: http://github.com/zqhxuyuan/2015/06/20/2015-06-20-Apache-Spark/　　作者: 任何忧伤,都抵不过世界的美丽" title="点击复制文章链接">
        <i class="fa fa-clipboard"></i>
    </span>
  </p>
  <p><span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" title="中国大陆 (CC BY-NC-SA 3.0 CN)">"署名-非商用-相同方式共享 3.0"</a> 转载请保留原文链接及作者。</p>
  <script src="/js/clipboard.min.js"></script>
  <script> var clipboard = new Clipboard('.btn'); </script>
</div>
<style type="text/css">
  .copyright p .btn {
    margin-left: 1em;
  }
  .copyright:hover p .btn::after {
    content: "复制"
  }
  .copyright p .btn:hover {
      color: gray;
      cursor: pointer;
    };
</style>



<nav id="article-nav">
  
    <div id="article-nav-newer" class="article-nav-title">
      <a href="/2015/07/06/2015-07-06-Spark-Streamming/">
        Spark Stramming入门
      </a>
    </div>
  
  
    <div id="article-nav-older" class="article-nav-title">
      <a href="/2015/05/01/hello-world/">
        Hello World
      </a>
    </div>
  
</nav>

  
  
    <div class ="post-donate">
	<br/>
	<p>
    <div id="donate_board" class="donate_bar center">
        <a id="btn_donate" class="btn_donate" href="javascript:;" title="打赏"></a>
        <span class="donate_txt">
           &uarr;<br>
		   这么长的文章我都读完了，看起来作者写的很辛苦哇！给作者赏杯咖啡喝，支持作者继续写作😁
        </span>
        <br>
    </div>  
	<div id="donate_guide" class="donate_bar center hidden" >
		<img src="/img/zhifubao.png" alt="支付宝打赏"> 
		<img src="/img/weixin.png" alt="微信打赏">  
    </div>
	<script type="text/javascript">
		document.getElementById('btn_donate').onclick = function(){
			$('#donate_board').addClass('hidden');
			$('#donate_guide').removeClass('hidden');
		}
	</script>
</div>
  
</article>

<!-- 默认显示文章目录，在文章---前输入toc: false关闭目录 -->
<!-- Show TOC and tocButton in default, Hide TOC via putting "toc: false" before "---" at [post].md -->
<div id="toc" class="toc-article">
<strong class="toc-title">文章目录</strong>
<ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark本地模式快速（十秒钟）入门"><span class="toc-number">1.</span> <span class="toc-text">Spark本地模式快速（十秒钟）入门</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Jobs_&_Stages"><span class="toc-number">1.1.</span> <span class="toc-text">Jobs & Stages</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark_Standalone集群安装（30分钟~1小时）"><span class="toc-number">2.</span> <span class="toc-text">Spark Standalone集群安装（30分钟~1小时）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#问题"><span class="toc-number">2.1.</span> <span class="toc-text">问题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark_Standalone_HA（30分钟）"><span class="toc-number">3.</span> <span class="toc-text">Spark Standalone HA（30分钟）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark_SQL"><span class="toc-number">4.</span> <span class="toc-text">Spark SQL</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Hive_on_Spark"><span class="toc-number">4.1.</span> <span class="toc-text">Hive on Spark</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SparkSQL_&_thrift"><span class="toc-number">4.2.</span> <span class="toc-text">SparkSQL & thrift</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Spark-SQL配置参数"><span class="toc-number">4.3.</span> <span class="toc-text">Spark-SQL配置参数</span></a></li></ol></li></ol>
</div>
<style type="text/css">
  .left-col .switch-btn {
    display: none;
  }
  .left-col .switch-area {
    display: none;
  }
</style>

<input type="button" id="tocButton" value="隐藏目录"  title="点击按钮隐藏或者显示文章目录">
<script type="text/javascript">
  var toc_button= document.getElementById("tocButton");
  var toc_div= document.getElementById("toc");
  /* Show or hide toc when click on tocButton.
  通过点击设置的按钮显示或者隐藏文章目录.*/
  toc_button.onclick=function(){
  if(toc_div.style.display=="none"){
  toc_div.style.display="block";
  toc_button.value="隐藏目录";
  document.getElementById("switch-btn").style.display="none";
  document.getElementById("switch-area").style.display="none";
  }
  else{
  toc_div.style.display="none";
  toc_button.value="显示目录";
  document.getElementById("switch-btn").style.display="block";
  document.getElementById("switch-area").style.display="block";
  }
  }
    if ($(".toc").length < 1) {
        $("#toc").css("display","none");
        $("#tocButton").css("display","none");
        $(".switch-btn").css("display","block");
        $(".switch-area").css("display","block");
    }
</script>


    <style>
        .toc {
            white-space: nowrap;
            overflow-x: hidden;
        }
    </style>

    <script>
        $(document).ready(function() {
            $(".toc li a").mouseover(function() {
                var title = $(this).attr('href');
                $(this).attr("title", title);
            });
        })
    </script>




<div class="share">
	<div class="bdsharebuttonbox">
	<a href="#" class="bds_more" data-cmd="more"></a>
	<a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
	<a href="#" class="bds_sqq" data-cmd="sqq" title="分享给 QQ 好友"></a>
	<a href="#" class="bds_copy" data-cmd="copy" title="复制网址"></a>
	<a href="#" class="bds_mail" data-cmd="mail" title="通过邮件分享"></a>
	<a href="#" class="bds_weixin" data-cmd="weixin" title="生成文章二维码"></a>
	</div>
	<script>
	window._bd_share_config={
		"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
	</script>
</div>



<div class="duoshuo" id="comments">
	<!-- 多说评论框 start -->
	<div class="ds-thread" data-thread-key="2015/06/20/2015-06-20-Apache-Spark/" data-title="Apache Spark入门" data-url="http://github.com/zqhxuyuan/2015/06/20/2015-06-20-Apache-Spark/"></div>
	<!-- 多说评论框 end -->
	<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
	<script type="text/javascript">
	var duoshuoQuery = {short_name:"zqhxuyuan"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		 || document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>
	<!-- 多说公共JS代码 end -->
</div>






    <style type="text/css">
    #scroll {
      display: none;
    }
    </style>
    <div class="scroll">
    <a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments" title="查看评论"><i class="fa fa-comments-o"></i></a>
    <a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a>
    </div>


  
  
    
    <div  class="post-nav-button">
    <a href="/2015/07/06/2015-07-06-Spark-Streamming/" title="上一篇: Spark Stramming入门">
    <i class="fa fa-angle-left"></i>
    </a>
    <a href="/2015/05/01/hello-world/" title="下一篇: Hello World">
    <i class="fa fa-angle-right"></i>
    </a>
    </div>
  



    
        <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
        <script>
        var yiliaConfig = {
        fancybox: true,
        mathjax: true,
        animate: true,
        isHome: false,
        isPost: true,
        isArchive: false,
        isTag: false,
        isCategory: false,
        open_in_new: false
        }
        </script>
        
</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
      <div class="footer-left">
        &copy; 2017 任何忧伤,都抵不过世界的美丽
      </div>
        <div class="footer-right">
          <a href="http://hexo.io/" target="_blank" title="快速、简洁且高效的静态博客框架">Hexo</a>  Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="简而不减双栏 Hexo 博客主题">Yelee</a> by MOxFIVE
        </div>
    </div>
    <div class="visit">
      <span id="busuanzi_container_site_pv" style='display:none'>
        <span id="site-visit" >本站到访数: 
        <span id="busuanzi_value_site_uv"></span>
        </span>
      </span>
      <span id="busuanzi_container_page_pv" style='display:none'>
        <span id="page-visit">, 本页阅读量: 
        <span id="busuanzi_value_page_pv"></span>
        </span>
      </span>
    </div>
  </div>
</footer>
    </div>
    

<script src="http://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js" type="text/javascript"></script>
<script src="/js/main.js" type="text/javascript"></script>

<script>
  var backgroundnum = 5;
  var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));

  $("body").css({"background": backgroundimg, "background-attachment": "fixed", "background-size": "cover"});
</script>


<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-80646710-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<div class="scroll" id="scroll">
<a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a>
<a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a>
</div>

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
  </div>
</body>
</html>